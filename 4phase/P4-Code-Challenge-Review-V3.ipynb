{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Phase 4 Code Challenge Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Pipelines and gridsearching\n",
    "- Ensemble Methods\n",
    "- Natural Language Processing\n",
    "- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.call import call_on_students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Pipelines and Gridsearching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the benefits of using a pipline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does a gridsearch achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Set up a pipeline with a scaler and a logistic regression model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Don't worry for now about a train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Split the data into train and test and then gridsearch over pipelines like the one you just built to find the best-performing model. Try C (inverse regularization) values of 10, 1, and 0.1. Try out the best estimator on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What sorts of ensembling methods have we looked at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is random about a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What hyperparameters of a random forest might it be useful to tune? How so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Build a random forest model on the breast cancer dataset that predicts whether the tumor is malignant (target = 1). Make sure you do a train-test split!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 23
   },
   "source": [
    "# 3) Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Concepts\n",
    "\n",
    "### Some Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 24
   },
   "outputs": [],
   "source": [
    "# Each sentence is a document\n",
    "sentence_one = \"Harry Potter is the best young adult book about wizards\"\n",
    "sentence_two = \"Um, EXCUSE ME! Ever heard of Earth Sea?\"\n",
    "sentence_three = \"I only like to read non-fiction.  It makes me a better person.\"\n",
    "\n",
    "# The corpus is composed of all of the documents\n",
    "corpus = [sentence_one, sentence_two, sentence_three]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: NLP Pre-processing\n",
    "\n",
    "List at least three steps you can take to turn raw text like this into something that would be semantically valuable (aka ready to turn into numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- Lowercase (standardize case)\n",
    "- Remove stopwords (really common words that likely have no semantic value)\n",
    "- Stem or lemmatize to remove prefixes/suffixes/grammer bits\n",
    "- Remove punctuation\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Describe what vectorized text would look like as a dataframe.\n",
    "\n",
    "If you vectorize the above corpus, what would the rows and columns be in the resulting dataframe (aka document term matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- Columns: every word/token in the dataset/corpus\n",
    "- Rows: the documents you're vectorizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: What does TF-IDF do?\n",
    "\n",
    "Also, what does TF-IDF stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 25
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- TF-IDF: term frequency inverse document frequency\n",
    "- TF-IDF is a vectorizer that takes into account the rarity of the words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 33
   },
   "source": [
    "## NLP in Code\n",
    "\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 34
   },
   "outputs": [],
   "source": [
    "# New section, new data\n",
    "policies = pd.read_csv('data/2020_policies_feb_24.csv')\n",
    "\n",
    "def warren_not_warren(label):\n",
    "    \n",
    "    '''Make label a binary between Elizabeth Warren\n",
    "    speeches and speeches from all other candidates'''\n",
    "    \n",
    "    if label =='warren':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "policies['candidate'] = policies['candidate'].apply(warren_not_warren)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 35
   },
   "source": [
    "The dataframe loaded above consists of policies of 2020 Democratic presidential hopefuls. The `policy` column holds text describing the policies themselves.  The `candidate` column indicates whether it was or was not an Elizabeth Warren policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 36
   },
   "outputs": [],
   "source": [
    "policies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 37
   },
   "source": [
    "The documents for activity are in the `policy` column, and the target is candidate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Import the Relevant Class, Then Instantiate and Fit a Count Vectorizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First! Train-test split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Code here to train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(policies['policy'], policies['candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate it\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit it\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Vectorize Your Text, Then Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 42
   },
   "outputs": [],
   "source": [
    "# Code here to transform train and test sets with the vectorizer\n",
    "X_tr_vec = vectorizer.transform(X_train)\n",
    "X_te_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 44
   },
   "outputs": [],
   "source": [
    "# Importing the classifier...\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Code here to instantiate and fit a Random Forest model\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_tr_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to evaluate your model on the test set\n",
    "rfc.score(X_te_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "# 4) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 1: Describe how the K-Means algorithm updates its cluster centers after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:\n",
    "\n",
    "- You set the number of cluster centers (K) - algorithm randomly starts with that number of cluster centers (in random spots!)\n",
    "- The algorithm calculates the distance between the centers and each observation and assigns the observation to the closest cluster center to create the first iteration of clusters\n",
    "- The algorithm then takes all the observations assigned to each cluster, and moves that cluster center to be at the exact actual center (mean) of the newly created cluster\n",
    "- Repeat! Until the cluster centers stop moving (or tolerance is met - some parameters in the implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 2: What is inertia, and how does K-Means use inertia to determine the best estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Please also describe the method you can use to evaluate clustering using inertia.\n",
    "\n",
    "Documentation, for reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "- Inertia measures the distance between each point and its center - the idea is that better clusters are more tightly concentrated\n",
    "- KMeans tries to minimize inertia when choosing cluster centers\n",
    "- Method to evaluate - elbow plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 3: What other metric do we have to score the clusters which are formed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Describe the difference between it and inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "- Silhouette score\n",
    "- Difference between silhouette score and inertia: silhouette score tries to maximize similarity within groups and maximize distances between clusters, while inertia just looks within each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "## Clustering in Code with Heirarchical Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "After the above conceptual review of KMeans, let's practice coding with agglomerative clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# New dataset for this section!\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 4: Prepare our Data for Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "What steps do we need to take to preprocess our data effectively?\n",
    "\n",
    "- scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Code to preprocess the data\n",
    "k_scaler = StandardScaler()\n",
    "\n",
    "# Name the processed data X_processed\n",
    "X_processed = k_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 5: Import the Relevant Class, Then Instantiate and Fit a Hierarchical Agglomerative Clustering Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "Let's use `n_clusters = 2` to start (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# Import the relevent clustering algorithm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Instantiate\n",
    "cluster = AgglomerativeClustering(n_clusters=2)\n",
    "# Fit the object\n",
    "cluster.fit(X_processed)\n",
    "\n",
    "# Calculate a silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(X_processed, cluster.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "### 6: Write a Function to Test Different Options for `n_clusters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "index": 83
   },
   "source": [
    "The function should take in the number for `n_clusters` and the data to cluster, fit a new clustering model using that parameter to the data, print the silhouette score, then return the labels attribute from the fit clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "index": 83
   },
   "outputs": [],
   "source": [
    "def test_n_for_clustering(n, data):\n",
    "    \"\"\" \n",
    "    Tests different numbers for the hyperparameter n_clusters\n",
    "    Prints the silhouette score for that clustering model\n",
    "    Returns the labels that are output from the clustering model\n",
    "\n",
    "    Parameters: \n",
    "    -----------\n",
    "    n: float object\n",
    "        number of clusters to use in the agglomerative clustering model\n",
    "    data: Pandas DataFrame or array-like object\n",
    "        Data to cluster\n",
    "\n",
    "    Returns: \n",
    "    --------\n",
    "    labels: array-like object\n",
    "        Labels attribute from the clustering model\n",
    "    \"\"\"\n",
    "    # Create the new clustering model\n",
    "    cluster = AgglomerativeClustering(n_clusters=n)\n",
    "    \n",
    "    # Fit the new clustering model\n",
    "    cluster.fit(data)\n",
    "\n",
    "    # Print the silhouette score\n",
    "    print(silhouette_score(data, cluster.labels_))\n",
    "    \n",
    "    # Return the labels attribute from the fit clustering model\n",
    "    return cluster.labels_\n",
    "\n",
    "# Testing your function\n",
    "\n",
    "for n in range(2, 9):\n",
    "    test_n_for_clustering(n, X_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
